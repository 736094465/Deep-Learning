{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7502\n",
      "1047\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "dir = \"D:\\\\01\\\\0Food challenge\\\\Tofu and potatoes\"\n",
    "train_dir = \"D:\\\\01\\\\0Food challenge\\\\Tofu and potatoes\\\\train\"\n",
    "test_dir = \"D:\\\\01\\\\0Food challenge\\\\Tofu and potatoes\\\\test\"\n",
    "\n",
    "print(len(os.listdir(train_dir)))\n",
    "print(len(os.listdir(test_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label\n",
       "0   0      1\n",
       "1   1      0\n",
       "2   2      0\n",
       "3   3      0\n",
       "4   4      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "y_train_df = pd.read_csv(os.path.join(dir,\"train.csv\"),names=[\"id\",\"label\"])\n",
    "y_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    4533\n",
       "0    2969\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "Y_train = y_train_df[\"label\"]\n",
    "\n",
    "g = sns.countplot(Y_train)\n",
    "Y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.26338835 0.82748732]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "\n",
    "classweight = class_weight.compute_class_weight(\"balanced\",np.unique(Y_train),Y_train)\n",
    "print(classweight)\n",
    "# class_weight = {[1.26338835 0.82748732]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义读取图片函数\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def get_img(file_path,img_rows,img_cols):\n",
    "  \n",
    "    img = cv2.imread(file_path)\n",
    "    img = cv2.resize(img,(img_rows,img_cols))\n",
    "    if img.shape[2] == 1:\n",
    "        img = np.dstack([img,img,img])\n",
    "    else:\n",
    "        img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "    img = img.astype(np.float32)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.1.1) C:\\projects\\opencv-python\\opencv\\modules\\core\\src\\alloc.cpp:72: error: (-4:Insufficient memory) Failed to allocate 1440000 bytes in function 'cv::OutOfMemoryError'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-652ee2898f1e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mimg_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx_train_img_path\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mimg_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\".jpg\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_img\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mimg_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m224\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m224\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-f2ad0755fa6e>\u001b[0m in \u001b[0;36mget_img\u001b[1;34m(file_path, img_rows, img_cols)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_img\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mimg_rows\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mimg_cols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_rows\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mimg_cols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.1.1) C:\\projects\\opencv-python\\opencv\\modules\\core\\src\\alloc.cpp:72: error: (-4:Insufficient memory) Failed to allocate 1440000 bytes in function 'cv::OutOfMemoryError'\n"
     ]
    }
   ],
   "source": [
    "x_train_img_path = y_train_df[\"id\"]\n",
    "\n",
    "# 加载训练集\n",
    "X_train = []\n",
    "for img_name in x_train_img_path:\n",
    "    img_name = str(img_name) + \".jpg\"\n",
    "    img = get_img(os.path.join(train_dir,img_name),224,224)\n",
    "    X_train.append(img)\n",
    "\n",
    "X_train = np.array(X_train,np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.imshow(X_train[0]/255)\n",
    "print(Y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对标签数据进行one-hot编码\n",
    "n_classes = len(np.unique(Y_train))\n",
    "\n",
    "from keras.utils import np_utils\n",
    "Y_train = np_utils.to_categorical(Y_train,n_classes)\n",
    "print(\"Shape after one-hot encoding:\",Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分数据集\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train,x_valid,y_train,y_valid = train_test_split(X_train,Y_train,test_size=0.3,random_state=2019)\n",
    "\n",
    "\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(x_valid.shape)\n",
    "print(y_valid.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入开发需要的库\n",
    "from keras import optimizers, Input\n",
    "from keras.applications import  imagenet_utils\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import *\n",
    "from keras.applications import *\n",
    "\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制训练过程中的 loss 和 acc 变化曲线\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def history_plot(history_fit):\n",
    "    plt.figure(figsize=(12,6))\n",
    "    \n",
    "    # summarize history for accuracy\n",
    "    plt.subplot(121)\n",
    "    plt.plot(history_fit.history[\"acc\"])\n",
    "    plt.plot(history_fit.history[\"val_acc\"])\n",
    "    plt.title(\"model accuracy\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.legend([\"train\", \"valid\"], loc=\"upper left\")\n",
    "    \n",
    "    # summarize history for loss\n",
    "    plt.subplot(122)\n",
    "    plt.plot(history_fit.history[\"loss\"])\n",
    "    plt.plot(history_fit.history[\"val_loss\"])\n",
    "    plt.title(\"model loss\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.legend([\"train\", \"test\"], loc=\"upper left\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型编译\n",
    "import keras.backend as K\n",
    "\n",
    "# focal loss \n",
    "def focal_loss(alpha=0.25,gamma=2.0):\n",
    "    def focal_crossentropy(y_true, y_pred):\n",
    "        bce = K.binary_crossentropy(y_true, y_pred)\n",
    "        \n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1.- K.epsilon())\n",
    "        p_t = (y_true*y_pred) + ((1-y_true)*(1-y_pred))\n",
    "        \n",
    "        alpha_factor = 1\n",
    "        modulating_factor = 1\n",
    "\n",
    "        alpha_factor = y_true*alpha + ((1-alpha)*(1-y_true))\n",
    "        modulating_factor = K.pow((1-p_t), gamma)\n",
    "\n",
    "        # compute the final loss and return\n",
    "        return K.mean(alpha_factor*modulating_factor*bce, axis=-1)\n",
    "    return focal_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tune 模型\n",
    "def fine_tune_model(model, optimizer, batch_size, epochs, freeze_num):\n",
    "    '''\n",
    "    discription: 对指定预训练模型进行fine-tune，并保存为.hdf5格式\n",
    "    \n",
    "    MODEL：传入的模型，VGG16， ResNet50, ...\n",
    "\n",
    "    optimizer: fine-tune all layers 的优化器, first part默认用adadelta\n",
    "    batch_size: 每一批的尺寸，建议32/64/128\n",
    "    epochs: fine-tune all layers的代数\n",
    "    freeze_num: first part冻结卷积层的数量\n",
    "    '''\n",
    "\n",
    "    # datagen = ImageDataGenerator(\n",
    "    #     rescale=1.255,\n",
    "    #     # shear_range=0.2,\n",
    "    #     # zoom_range=0.2,\n",
    "    #     # horizontal_flip=True,\n",
    "    #     # vertical_flip=True,\n",
    "    #     # fill_mode=\"nearest\"\n",
    "    #   )\n",
    "    \n",
    "    # datagen.fit(X_train)\n",
    "    \n",
    "    \n",
    "#     # first: 仅训练全连接层（权重随机初始化的）\n",
    "#     # 冻结所有卷积层\n",
    "    \n",
    "#     for layer in model.layers[:freeze_num]:\n",
    "#         layer.trainable = False\n",
    "    \n",
    "#     model.compile(optimizer=optimizer, \n",
    "#                   loss=\"categorical_crossentropy\",\n",
    "#                   metrics=[\"accuracy\"])\n",
    "\n",
    "#     # model.fit_generator(datagen.flow(x_train,y_train,batch_size=batch_size),\n",
    "#     #                     steps_per_epoch=len(x_train)/32,\n",
    "#     #                     epochs=3,\n",
    "#     #                     shuffle=True,\n",
    "#     #                     verbose=1,\n",
    "#     #                     datagen.flow(x_valid, y_valid))\n",
    "#     model.fit(X_train,\n",
    "#          Y_train,\n",
    "#          batch_size=batch_size,\n",
    "#          epochs=3,\n",
    "#          shuffle=True,\n",
    "#          verbose=1,\n",
    "#          validation_split=0.3\n",
    "#         )\n",
    "#     print('Finish step_1')\n",
    "    \n",
    "    \n",
    "    # second: fine-tune all layers\n",
    "    for layer in model.layers[:]:\n",
    "        layer.trainable = True\n",
    "    \n",
    "    rc = ReduceLROnPlateau(monitor=\"val_loss\",\n",
    "                factor=0.2,\n",
    "                patience=3,\n",
    "                verbose=1,\n",
    "                mode='min')\n",
    "\n",
    "    model_name = model.name  + \".hdf5\"\n",
    "    mc = ModelCheckpoint(model_name, \n",
    "               monitor=\"val_loss\", \n",
    "               save_best_only=True,\n",
    "               verbose=1,\n",
    "               mode='min')\n",
    "    el = EarlyStopping(monitor=\"val_loss\",\n",
    "              min_delta=0,\n",
    "              patience=5,\n",
    "              verbose=1,\n",
    "              restore_best_weights=True)\n",
    "    \n",
    "    model.compile(optimizer=optimizer, \n",
    "           loss='categorical_crossentropy', \n",
    "           metrics=[\"accuracy\"])\n",
    "\n",
    "    # history_fit = model.fit_generator(datagen.flow(x_train,y_train,batch_size=32),\n",
    "    #                                  steps_per_epoch=len(x_train)/32,\n",
    "    #                                  epochs=epochs,\n",
    "    #                                  shuffle=True,\n",
    "    #                                  verbose=1,\n",
    "    #                                  callbacks=[mc,rc,el],\n",
    "    #                                  datagen.flow(x_valid, y_valid))\n",
    "    history_fit = model.fit(X_train,\n",
    "                 Y_train,\n",
    "                 batch_size=batch_size,\n",
    "                 epochs=epochs,\n",
    "                 shuffle=True,\n",
    "                 verbose=1,\n",
    "                 validation_split=0.3,\n",
    "                 callbacks=[mc,rc,el],\n",
    "                 class_weight={0:1.26338835,1:0.82748732})\n",
    "    \n",
    "    print('Finish fine-tune')\n",
    "    return history_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG16模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个VGG16的模型\n",
    "def vgg16_model(img_rows,img_cols):\n",
    "    x = Input(shape=(img_rows, img_cols, 3))\n",
    "    x = Lambda(imagenet_utils.preprocess_input)(x)\n",
    "    base_model = VGG16(input_tensor=x,weights=\"imagenet\",include_top=False, pooling='avg')\n",
    "    x = base_model.output\n",
    "    x = Dense(1024,activation=\"relu\",name=\"fc1\")(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    predictions = Dense(n_classes,activation=\"softmax\",name=\"predictions\")(x)  \n",
    "    vgg16_model = Model(inputs=base_model.input,outputs=predictions,name=\"vgg16\")\n",
    "    \n",
    "    return vgg16_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建VGG16模型\n",
    "img_rows, img_cols = 224, 224\n",
    "vgg16_model = vgg16_model(img_rows,img_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\software\\Anaconda\\anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 5251 samples, validate on 2251 samples\n",
      "Epoch 1/30\n",
      "5251/5251 [==============================] - 112s 21ms/step - loss: 1.5082 - acc: 0.7513 - val_loss: 0.4705 - val_acc: 0.8801\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.47049, saving model to vgg16.hdf5\n",
      "Epoch 2/30\n",
      "5251/5251 [==============================] - 101s 19ms/step - loss: 0.7746 - acc: 0.8497 - val_loss: 0.3659 - val_acc: 0.9054\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.47049 to 0.36587, saving model to vgg16.hdf5\n",
      "Epoch 3/30\n",
      "5251/5251 [==============================] - 103s 20ms/step - loss: 0.5492 - acc: 0.8787 - val_loss: 0.2995 - val_acc: 0.9156\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36587 to 0.29953, saving model to vgg16.hdf5\n",
      "Epoch 4/30\n",
      "5251/5251 [==============================] - 100s 19ms/step - loss: 0.4262 - acc: 0.8960 - val_loss: 0.2683 - val_acc: 0.9174\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.29953 to 0.26831, saving model to vgg16.hdf5\n",
      "Epoch 5/30\n",
      "5251/5251 [==============================] - 97s 19ms/step - loss: 0.3335 - acc: 0.9063 - val_loss: 0.2650 - val_acc: 0.9236\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.26831 to 0.26500, saving model to vgg16.hdf5\n",
      "Epoch 6/30\n",
      "5251/5251 [==============================] - 96s 18ms/step - loss: 0.2619 - acc: 0.9206 - val_loss: 0.2591 - val_acc: 0.9174\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.26500 to 0.25914, saving model to vgg16.hdf5\n",
      "Epoch 7/30\n",
      "5251/5251 [==============================] - 93s 18ms/step - loss: 0.2312 - acc: 0.9276 - val_loss: 0.2314 - val_acc: 0.9231\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.25914 to 0.23138, saving model to vgg16.hdf5\n",
      "Epoch 8/30\n",
      "5251/5251 [==============================] - 88s 17ms/step - loss: 0.1893 - acc: 0.9370 - val_loss: 0.2255 - val_acc: 0.9280\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.23138 to 0.22552, saving model to vgg16.hdf5\n",
      "Epoch 9/30\n",
      "5251/5251 [==============================] - 88s 17ms/step - loss: 0.1604 - acc: 0.9448 - val_loss: 0.2132 - val_acc: 0.9303\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.22552 to 0.21324, saving model to vgg16.hdf5\n",
      "Epoch 10/30\n",
      "5251/5251 [==============================] - 88s 17ms/step - loss: 0.1524 - acc: 0.9471 - val_loss: 0.2001 - val_acc: 0.9334\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.21324 to 0.20013, saving model to vgg16.hdf5\n",
      "Epoch 11/30\n",
      "5251/5251 [==============================] - 88s 17ms/step - loss: 0.1241 - acc: 0.9539 - val_loss: 0.2082 - val_acc: 0.9276\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.20013\n",
      "Epoch 12/30\n",
      "5251/5251 [==============================] - 87s 17ms/step - loss: 0.1191 - acc: 0.9530 - val_loss: 0.2042 - val_acc: 0.9365\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.20013\n",
      "Epoch 13/30\n",
      "5251/5251 [==============================] - 88s 17ms/step - loss: 0.1051 - acc: 0.9627 - val_loss: 0.1981 - val_acc: 0.9369\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.20013 to 0.19811, saving model to vgg16.hdf5\n",
      "Epoch 14/30\n",
      "5251/5251 [==============================] - 87s 17ms/step - loss: 0.0910 - acc: 0.9642 - val_loss: 0.1999 - val_acc: 0.9320\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.19811\n",
      "Epoch 15/30\n",
      "5251/5251 [==============================] - 87s 17ms/step - loss: 0.0857 - acc: 0.9655 - val_loss: 0.2060 - val_acc: 0.9267\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.19811\n",
      "Epoch 16/30\n",
      "5251/5251 [==============================] - 87s 17ms/step - loss: 0.0755 - acc: 0.9718 - val_loss: 0.1868 - val_acc: 0.9360\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.19811 to 0.18685, saving model to vgg16.hdf5\n",
      "Epoch 17/30\n",
      "5251/5251 [==============================] - 87s 17ms/step - loss: 0.0639 - acc: 0.9771 - val_loss: 0.2034 - val_acc: 0.9311\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.18685\n",
      "Epoch 18/30\n",
      "5251/5251 [==============================] - 87s 16ms/step - loss: 0.0586 - acc: 0.9764 - val_loss: 0.1978 - val_acc: 0.9307\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.18685\n",
      "Epoch 19/30\n",
      "5251/5251 [==============================] - 87s 17ms/step - loss: 0.0532 - acc: 0.9802 - val_loss: 0.1941 - val_acc: 0.9369\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.18685\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "Epoch 20/30\n",
      "5251/5251 [==============================] - 87s 17ms/step - loss: 0.0425 - acc: 0.9844 - val_loss: 0.1913 - val_acc: 0.9369\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.18685\n",
      "Epoch 21/30\n",
      "5251/5251 [==============================] - 88s 17ms/step - loss: 0.0434 - acc: 0.9834 - val_loss: 0.1926 - val_acc: 0.9343\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.18685\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00021: early stopping\n",
      "Finish fine-tune\n",
      "Wall time: 32min 13s\n"
     ]
    }
   ],
   "source": [
    "optimizer = optimizers.Adam(lr=0.0001)\n",
    "batch_size = 32\n",
    "epochs = 30\n",
    "freeze_num = 21\n",
    "\n",
    "\n",
    "%time vgg16_history = fine_tune_model(vgg16_model,optimizer,batch_size,epochs,freeze_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EfficentNet模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入Efficient模块\n",
    "from efficientnet.keras import EfficientNetB3\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个EfficientNet模型\n",
    "def efficient_model(img_rows,img_cols):\n",
    "    K.clear_session()\n",
    "    x = Input(shape=(img_rows,img_cols,3))\n",
    "    x = Lambda(imagenet_utils.preprocess_input)(x)\n",
    "    \n",
    "    base_model = EfficientNetB3(input_tensor=x,weights=\"imagenet\",include_top=False,pooling=\"avg\")\n",
    "    x = base_model.output\n",
    "    x = Dense(32,activation=\"relu\",name=\"fc1\")(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    predictions = Dense(n_classes,activation=\"softmax\",name=\"predictions\")(x)\n",
    "    eB_model = Model(inputs=base_model.input,outputs=predictions,name=\"eB3\")\n",
    "    \n",
    "    return eB_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\software\\Anaconda\\anaconda\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:95: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\software\\Anaconda\\anaconda\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:98: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\software\\Anaconda\\anaconda\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:102: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\software\\Anaconda\\anaconda\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\software\\Anaconda\\anaconda\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\software\\Anaconda\\anaconda\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\software\\Anaconda\\anaconda\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# 创建Efficient模型\n",
    "img_rows,img_cols=224,224\n",
    "eB_model = efficient_model(img_rows,img_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5251 samples, validate on 2251 samples\n",
      "Epoch 1/30\n",
      "5251/5251 [==============================] - 678s 129ms/step - loss: 0.1007 - acc: 0.9665 - val_loss: 0.3112 - val_acc: 0.9271\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.31122, saving model to eB3.hdf5\n",
      "Epoch 2/30\n",
      "5251/5251 [==============================] - 652s 124ms/step - loss: 0.0892 - acc: 0.9686 - val_loss: 0.3130 - val_acc: 0.9249\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.31122\n",
      "Epoch 3/30\n",
      "5251/5251 [==============================] - 647s 123ms/step - loss: 0.0679 - acc: 0.9773 - val_loss: 0.2228 - val_acc: 0.9471\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.31122 to 0.22276, saving model to eB3.hdf5\n",
      "Epoch 4/30\n",
      "5251/5251 [==============================] - 648s 123ms/step - loss: 0.0692 - acc: 0.9766 - val_loss: 0.4716 - val_acc: 0.8281\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.22276\n",
      "Epoch 5/30\n",
      "5251/5251 [==============================] - 645s 123ms/step - loss: 0.0617 - acc: 0.9796 - val_loss: 0.3340 - val_acc: 0.9143\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.22276\n",
      "Epoch 6/30\n",
      "5251/5251 [==============================] - 646s 123ms/step - loss: 0.0576 - acc: 0.9775 - val_loss: 0.2797 - val_acc: 0.9445\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.22276\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "Epoch 7/30\n",
      "5251/5251 [==============================] - 651s 124ms/step - loss: 0.0280 - acc: 0.9909 - val_loss: 0.2119 - val_acc: 0.9502\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.22276 to 0.21186, saving model to eB3.hdf5\n",
      "Epoch 8/30\n",
      "5251/5251 [==============================] - 651s 124ms/step - loss: 0.0121 - acc: 0.9954 - val_loss: 0.2843 - val_acc: 0.9525\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.21186\n",
      "Epoch 9/30\n",
      "5251/5251 [==============================] - 663s 126ms/step - loss: 0.0150 - acc: 0.9954 - val_loss: 0.2696 - val_acc: 0.9542\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.21186\n",
      "Epoch 10/30\n",
      "5251/5251 [==============================] - 649s 124ms/step - loss: 0.0100 - acc: 0.9970 - val_loss: 0.2627 - val_acc: 0.9520\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.21186\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 3.999999898951501e-06.\n",
      "Epoch 11/30\n",
      "5251/5251 [==============================] - 636s 121ms/step - loss: 0.0050 - acc: 0.9977 - val_loss: 0.3118 - val_acc: 0.9538\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.21186\n",
      "Epoch 12/30\n",
      "5251/5251 [==============================] - 635s 121ms/step - loss: 0.0069 - acc: 0.9983 - val_loss: 0.2854 - val_acc: 0.9534\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.21186\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00012: early stopping\n",
      "Finish fine-tune\n"
     ]
    }
   ],
   "source": [
    "optimizer = optimizers.Adam(lr=0.0001)\n",
    "batch_size = 2\n",
    "epochs = 30\n",
    "freeze_num = 379\n",
    "eB_model_history  = fine_tune_model(eB_model,optimizer,batch_size,epochs,freeze_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "x_test_img_path = os.listdir(test_dir)\n",
    "x_test_img_path = sorted(x_test_img_path,key = lambda i:int(re.match(r\"(\\d+)\",i).group()))\n",
    "\n",
    "x_test = []\n",
    "for img_name in x_test_img_path:\n",
    "    img = get_img(os.path.join(test_dir,img_name),224,224)\n",
    "    x_test.append(img)\n",
    "\n",
    "x_test = np.array(x_test,np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1047, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "eB_model.load_weights(\"./eB3.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = eB_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99832207, 0.00167793], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = np.argmax(y_pred,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "len = predict.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = np.arange(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"id\":id,\"predict\":predict})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"submit_efficient.csv\",index=None,header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
